---
title: "Cuestiones prácticas de VI"
author: "Irvin Rojas"
institute: "CIDE"
date: "7 de octubre de 2021"
mathspec: true
output:
  xaringan::moon_reader:
    seal: false
    chakra: "https://remarkjs.com/downloads/remark-latest.min.js"
    lib_dir: libs
    nature:
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["middle", "center"]
      ratio: "16:9"
      beforeInit: ["https://platform.twitter.com/widgets.js", "libs/cols_macro.js"]
      navigation:
      scroll: false
    css: [default, "libs/cide.css", metropolis-fonts, "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css", "https://use.fontawesome.com/releases/v5.7.2/css/all.css", "https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"]
include-before:
- '\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{s}}}{~}}'

---
class: title-slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.path = "figures/")

library(tidyverse)
library(janitor)
library(sandwich)
library(modelsummary)

xfun::pkg_load2(c('base64enc', 'htmltools', 'mime'))

```

```{css, echo = FALSE}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 60% !important;
}

```


.title[
# Clase 15. Cuestiones prácticas de VI
]
.subtitle[
## Econometría II
]
.author[
### Irvin Rojas <br> [rojasirvin.com](https://www.rojasirvin.com/) <br> [<i class="fab fa-github"></i>](https://github.com/rojasirvin) [<i class="fab fa-twitter"></i>](https://twitter.com/RojasIrvin) [<i class="ai ai-google-scholar"></i>](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&hl=en)
]

.affiliation[
### Centro de Investigación y Docencia Económicas <br> División de Economía
]

---

# Cuestiones prácticas de VI

.center[
<iframe width="560" height="315" src="https://www.youtube.com/embed/8s4pgc3DPN0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
]

---

# Cuestiones prácticas de VI

- Hemos aprendido que la forma general del estimador de MGM para MC2E es:

$$\hat{\beta}_{GMM}=(X'ZW_NZ'X)^{-1}X'ZW_NZ'y$$

- Y vimos también la forma general del estimador de la varianza:

$$\hat{V}(\hat{\beta}_{GMM})=N(X'ZW_NZ'X)^{-1}(X'ZW_N\hat{S}W_NZ'X)(X'ZW_NZ'X)^{-1}$$
---

# Estimador óptimo de MGM

- Para obtener el estimador óptimo escogemos una forma particular para la matriz de pesos:

$$W=\hat{S}^{-1}$$
- Y entonces el estimador de MGM se vuelve:

$$\hat{\beta}_{GMM,O}=(X'Z\hat{S}^{-1}Z'X)^{-1}X'Z\hat{S}^{-1}Z'y$$
- Y el estimador de varianza se simplifica a:

$$\hat{V}(\hat{\beta}_{GMM,O})=N(X'Z\hat{S}^{-1}Z'X)^{-1}$$
- En estos dos estimadores no asumimos nada sobre la forma de los errores

- Lo único que nos permitió pasar de la forma general al estimador óptimo es la elección de $W$

- Con esto obtenemos el estimador más eficiente

---

# Modelo sobreidentificado con posible heterocedasticidad

- Con errores heterocedásticos, se estima $\hat{S}$ en una primera etapa usando, por ejemplo $W=I$

- Luego se estima $\hat{\beta}_{MGM,O}$ resolviendo
  
$$Q_N(\theta)=\left(\frac{1}{N}\sum_i (y − X\beta)'Z\right)\hat{S}^{-1}\left(\frac{1}{N}\sum_i Z'(y − X\beta)\right)$$
- Y luego estimamos la varianza

$$\hat{V}(\hat{\beta}_{GMM,O})=N(X'Z\tilde{S}^{-1}Z'X)^{-1}$$
- La diferencia es que usamos los residuales de  $\hat{\beta}_{MGM,O}$ para construir $\tilde{S}$

- Este es el **estimador óptimo de MGM**

---

# Modelo sobreidentificado con homocedasticidad

- Con errores homocedásticos:

$$\hat{S}^{-1}=\left(\frac{1}{N}\sigma^2Z'Z\right)^{-1}$$
- Y entonces hacemos
$$W=\left(\frac{1}{N}Z'Z\right)^{-1}$$
- Con esta simplificación, el estimador de MGM es:

$$
\begin{aligned}
\hat{\beta}_{MC2E}&=(X'Z(Z'Z)^{-1}Z'X)^{-1}X'ZZ(Z'Z)^{-1}Z'y \\
&=(X'P_ZX)^{-1}X'P_Zy
\end{aligned}
$$
- Este es el **estimador de MC2E**, también llamado **estimador de variables instrumentales generalizado**

---

# Modelo sobreidentificado con homocedasticidad

- A $P_Z=Z(Z'Z)^{-1}Z'$ se le conoce como matriz de proyección

- Y la matriz de varianzas se simplifica a

$$\hat{V}(\hat{\beta}_{MC2E})=\sigma^2\left(X'P_z X\right)^{-1}$$
---

# Modelo exactamente identificado

- En el caso cuando $r=q$, es decir, tantos instrumentos como variables endógenas, $X'Z$ es una matriz cuadrada que puede ser invertida

- Entonces

$$(X'ZW_NZ'X)^{-1}(X'Z)^{-1}=(Z'X)^{-1}W_N(X'Z)^{-1}$$

- Sustituyendo esto en la forma general del estimador de MGM obtenemos:

$$\beta_{VI}=(Z'X)^{-1}Z'y$$
- Este es el estimador de **variables instrumentales**

- En otras palabras, el estimador de MGM es igual al de VI para cualquier matriz $W_N$

---

# Modelo exactamente identificado

- Con heterocedasticidad, tenemos una matriz de varianzas de la forma

$$\hat{V}(\hat{\beta}_{VI})=N(Z'X)^{−1}\hat{S}(X'Z)^{−1}$$
donde $\hat{S}=\frac{1}{N}\sum_i\hat{u}_i^2z_iz_i'$

- Y con homocedasticidad, la matriz de varianzas de nuevo es:

$$\hat{V}(\hat{\beta}_{VI})=\sigma ^2\left(X'P_z X\right)^{-1}$$
---

# Recapitulando

- Podemos vivir con las convenciones de Camerony Trivedi (2005)

- El **estimador de MGM** es el estimador para el caso general de método de momentos, cuales quiera que sean las formas de los momentos especificados

- El **estimador óptimo de MGM** ocurre cuando asumimos una forma particular para la matriz de pesos, $W=\hat{S}^{-1}$

- El **estimador óptimo de MGM** se emplea en el caso más general de modelos de variables instrumentales sobreidentificados con heterocedasticidad

- El **estimador de variables instrumentales generalizado** se obtiene cuando asumimos homocedasticidad en el modelo sobreidentificado y lleva el *apellido* generalizado porque es la generalización del estimador IV para el caso sobreidentificado

- El **estimador de variables instrumentales** surge en el modelo exactamente identificado

---

class: inverse, middle, center

# Prueba de Hausman

---

# Prueba de Hausman

- En general, las pruebas que comparan dos estimadores distintos se conocen como pruebas de Hausman, Wu-Hausman o Durbin-Wu-Hausman

- Consideremos dos estimadores $\tilde{\theta}$ y $\hat{\theta}$ que tienen la misma probabilidad límite bajo la $H_0$ pero que difieren bajo la $H_a$

$$
\begin{aligned}
H_0:\quad\quad p\lim(\tilde{\theta}-\hat{\theta})=0 \\
H_a:\quad\quad p\lim(\tilde{\theta}-\hat{\theta})\neq 0 \\
\end{aligned}
$$
- Construimos el estadístico de prueba $H$:

$$H=(\tilde{\theta}-\hat{\theta})'(\hat{V}(\tilde{\theta}-\hat{\theta}))^{-1}(\tilde{\theta}-\hat{\theta})\stackrel{a}{\sim}\chi^2(q)$$

- Se rechaza la $H_0$ si $H>\chi^2_{\alpha}(q)$

- La implementacón es un poco complicada dado que

$$\hat{V}(\tilde{\theta}-\hat{\theta})=\hat{V}(\tilde{\theta})-\hat{V}(\hat{\theta})-2cov(\tilde{\theta},\hat{\theta})$$

---

# Prueba de Hausman

- Con errores homocedásticos, el estimador de MCO es eficiente

- En ese caso, se puede mostrar que

$$H_{h}=(\tilde{\theta}-\hat{\theta})'(\hat{V}(\tilde{\theta})-\hat{V}(\hat{\theta}))^{-1}(\tilde{\theta}-\hat{\theta})\stackrel{a}{\sim}\chi^2(q)$$
que es fácil de calcular en el software

- Si no estamos dispuestos a asumir homocedasticidad, se requiere estimar $cov(\tilde{\theta},\hat{\theta})$, que se implementa en R y otros paquetes

--

- La prueba de Hausman puede usarse para comparar dos estimadores, uno más eficiente que otro

- La estimación de la prueba robusta puede complicarse en algunas aplicaciones, aunque como prueba de endogeneidad casi todo está disponible como funciones en R y otros paquetes

---

class: inverse, middle, center

# Prueba de sobreidentificación

---

# Prueba de sobreidentificación

- También conocida como prueba de Hansen, quien propuso la forma general de la prueba, o prueba de Sargan, quien propuso la forma particular para el modelo lineal de VI

- Es una prueba sobre qué tan cerca está de cumplirse la hipótesis nula de que $E(h(w,\theta_0))=0$

- Hansen (1982) define el estadístico de prueba como

$$J=\left(\frac{1}{N}\sum_i \hat{h}_i\right)'\hat{S}^{-1}\left(\frac{1}{N}\sum_i \hat{h}_i\right)\stackrel{a}{\sim}\chi^2(r-q)$$
- El estadístico $J$ es la función objetivo de MGM evaluada en $\hat{\theta}_{MGM}$

- Si el estadístico es gran, rechazamos la hipótesis de que las condiciones de momentos poblacionales se cumplen y se concluye que el estimador de MGM es inconsistente

---

# Prueba de sobreidentificación

- En el caso de variables instrumentales, el estadístico tiene la forma específica:

$$J=\hat{u}'Z\hat{S}^{-1}Z'\hat{u}$$
donde $\hat{u}=y-X'\hat{\beta}_{MGM}$

- Si se rechaza $H_0$, hay evidencia de que los instrumentos $z$ son exógenos (aunque también podría ser que haya una mala especificación del modelo)


---

class: inverse, middle, center

# Instrumentos débiles


---

# Instrumentos débiles

- Discusión intuitiva en Angrist & Pischke (MHE, 2009)

- El estimador de MCO tiene las propiedades de ser consistente e insesgado

  - En una muestra de tamaño arbitrario, la distribución del coeficiente de MCO está centrada en el coeficiente de  poblacional
  
--

- En cambio, el estimador de MC2E, aunque consistente, **es sesgado**

  - En muestras grandes el el estimador está *cerca* del coeficiente poblacional

--

- Esto tiene importantes consecuencias para la estimación y la inferencia


---

# Sesgo del estimador de MC2E

- Consideremos el modelo simple con un solo regresor endógeno $y=\beta x+ \eta$

- Supongamos que tenemos una matriz de instrumentos $Z$, por lo que la primera etapa es:

$$x=Z\pi+\xi$$

- El estimador de MCE es:

$$\hat{\beta}_{MC2E}=\beta+(x'P_Z x)^{-1}x'P_Z\eta$$

- Sustituyendo $x$:

$$\hat{\beta}_{MC2E}-\beta=(x'P_z x)^{-1}\pi'Z'\eta+(x'P_z x)^{-1}\xi'P_z\eta=sesgo_{Mc2E}$$

- No podemos calcular directamente el sesgo pues el operador esperanza es un operador lineal

- Angrist & Pischke (2009) aproximan el sesgo como.


$$E(\hat{\beta}_{MC2E}-\beta)\approx(E(x'P_z x))^{-1}E(\pi'Z'\eta)+(E(x'P_z x))^{-1}\xi'P_z\eta$$
---

# Sesgo del estimador de MC2E

- La expresión del sesgo puede reescribirse como

$$E(\hat{\beta}_{MC2E}-\beta)\approx\frac{\sigma_{\eta\xi}}{\sigma_{xi}^2}\frac{1}{F+1}$$

donde $\frac{\sigma_{\eta \xi}}{\sigma_{xi}^2}$ es el sesgo del estimador de MCO

--

- Cuando $\pi=0$, el sesgo de MC2E es el mismo que el de MCO

- Es decir, cuando $F$ es pequeña, el sesgo de MC2E se acerca al sesgo de MCO: el estimador de MC2E está sesgado hacia el de MCO cuando la primera etapa es débil

- Staiger & Stock (1997) mostraron con simulaciones que cuando $F>10$, el sesgo máximo en el estimador de MC2E es de 10%

- De aquí viene la regla de dedo frecuentemente usada para juzgar instrumentos débiles


---

# Recomendaciones prácticas

1. Reportar la primera etapa y ver si los coeficientes tienen sentido económico

1. Reportar el estadístico $F$ de la primera etapa para los instrumentos excluidos

1. Reportar los resultados usando un modelo exactamente identificado usando el *mejor* instrumento

1. Poner atención a la forma reducida, recordando que la forma reducida es proporcional al efecto causal de interés

> "Si no puedes ver la relación causal de interés en la forma reducida es porque probablemente no haya nada ahí."
>
> --- Angrist & Krueger (2001)


---

# Próxima sesión

- Comenzaremos a hablar sobre panel

  - CT, Capítulos 21 y 22

---

class: center, middle

Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**
