<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Máxima verosimilitud y mínimos cuadrados no lineales</title>
    <meta charset="utf-8" />
    <meta name="author" content="Irvin Rojas" />
    <script src="libs/header-attrs-2.9/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="libs/cide.css" type="text/css" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css" type="text/css" />
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" type="text/css" />
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class: title-slide



.title[
# Clase 4. Máxima verosimilitud y mínimos cuadrados no lineales
]
.subtitle[
## Econometría II
]
.author[
### Irvin Rojas &lt;br&gt; [rojasirvin.com](https://www.rojasirvin.com/) &lt;br&gt; [&lt;i class="fab fa-github"&gt;&lt;/i&gt;](https://github.com/rojasirvin) [&lt;i class="fab fa-twitter"&gt;&lt;/i&gt;](https://twitter.com/RojasIrvin) [&lt;i class="ai ai-google-scholar"&gt;&lt;/i&gt;](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&amp;hl=en)
]

.affiliation[
### Centro de Investigación y Docencia Económicas &lt;br&gt; División de Economía
]

---
# Agenda
  
1. Mostrar las propiedades de los estimadores de máxima verosimilitud como un tipo de estimadores M

1. Mostrar las propiedades de los estimadores de MCNL como un segundo tipo de estimadores M


---

class: inverse, middle, center


# Máxima verosimilitud

---

# Máxima verosimilitud

- El estimador de máxima verosimilitud es comúnmente usado en econometría

- Es el estimador más eficiente de entre los estimadores asintóticamente normales

- El principio de verosimilitud (Fisher, 1922) consiste en escoger el vector de parámetros que maximice la probabilidad de observar los datos

- En este contexto, consideramos a la función de masa de probabilidad o densidad `\(f(y,X|\theta)\)` como una función de `\(\theta\)`, dados unos datos `\((y,X)\)`

- Recordemos que el **estimador de maxima verosimilitud condicional** al máximo local que satisface la condición de primer orden:

`$$\frac{1}{N}\frac{\partial \mathcal{L}_N(\theta)}{\partial \theta }=\frac{1}{N}\sum_i\frac{\partial \ln f(y_i|x_i,\theta)}{\partial \theta}=0$$`
---

# Ejemplo Poisson

- Una de las aplicaciones que veremos más adelante trata el problema de modelos de conteo

- En estadística, los problemas de conteo muchas veces se modelan con la función de masa de probabilidad Poisson, que tiene un único parámetro `\(\lambda\)`:

`$$f(y|\lambda)=e^{\lambda}\lambda^y/y!$$`

con `\(y=0,1,2,\ldots\)`

- Sabemos además que `\(E(y)=V(y)=\lambda\)`

--

- Un modelo de regresión frecuentemente usado *parametriza* `\(\lambda\)` para que varíe de acuerdo a las caracteísticas `\(X\)` y un vector de parámetros: `\(\lambda=exp(x'\beta)\)`

- Así, el modelo de regresión Poisson puede escribirse como:

`$$f(y|x,\beta)=e^{exp(x'\beta)}exp(x'\beta)^y / y!$$`

---

# Ejemplo Poisson

- El problema de máxima verosimilitud con una muestra `\(\{(y_i,x_i\)}\)` con `\(N\)` datos consiste en encontre `\(\beta\)` que maximice la función de log verosimilitud

- La función de verosimilitud es la densidad conjunta

- Bajo independencia, la densidad conjunta es `\(\Pi_i f(y_i|x_i,\beta)\)`

- Y la función de log verosimilitud es el log del producto, es decir, la suma de los logs: `\(\sum_i \ln f(y_i|x_i,\beta)\)`

- En el caso Possion, la log densidad para la observación `\(i\)` es:

`$$\ln f(y_i|x_i,\beta)=-exp(x_i\beta)+y_ix_i'\beta-\ln y_i!$$`

---

# El estimador de MV

- El estimador `\(\hat{\beta}_{MV}\)` maximiza:

`$$Q_N(\beta)=\frac{1}{N}\sum_i\{-exp(x_i\beta)+y_ix_i'\beta-\ln y_i!\}$$`

- La condición de primer orden es:

`$$\frac{1}{N}\sum_i\left(y_i-exp(x_i'\beta)\right)x_i\big|_{\hat{\beta}}=0$$`
- Esta expresión no tiene una solución cerrada y usamos métodos numéricos para calcular `\(\hat{\beta}\)`


---

# Condiciones de regularidad

- Los resultados asintóticos de los estimadores M se cumplen para el estimador de MV si la densidad está bien especificada y el rango de `\(y\)` no depende de `\(\theta\)`

- Las dos condiciones de regularidad son:

1. El vector score tiene esperanza cero: `$$E_f\left(\frac{\partial \ln f(y|x,\theta)}{\partial \theta}\right)=\int\frac{\partial \ln f(y|x,\theta)}{\partial \theta}f(y|x,\theta)=0$$`

--

1. Con un vector score con esperanza cero, se cumple que: `$$-E_f\left(\frac{\partial^2\ln f(y|x,\theta)}{\partial \theta \partial \theta'}\right)=E_f\left(\frac{\partial \ln f(y|x,\theta)}{\partial \theta} \frac{\partial \ln f(y|x,\theta)}{\partial \theta'}\right)$$`


--

- Es relativamente fácil mostrar las condiciones de regularidad, por eso dejé este paso para la tarea



---

# Igualdad de la matriz de información

- La **matriz de información de Fisher** es la esperanza del producto exterior del score:

`$$\mathcal{I}=E\left(\frac{\partial \mathcal{L}_N(\theta)}{\partial \theta} \frac{\partial \mathcal{L}_N(\theta)}{\partial \theta'} \right)$$`
- Noten que `\(\mathcal{I}\)` es la varianza del score, dado que por la primera condición de regularidad, el score tiene expectativa cero

- El término *información* indica que si `\(\mathcal{I}\)` es grande, entonces cambios en `\(\theta\)` tiene cambios grandes en la log verosimilitud, revelando mucha información sobre `\(\theta\)`


--

- En nuestro problema de MV, la segunda condición de regularidad implica:

 `$$\mathcal{I}=E_f\left(\frac{\partial \mathcal{L}_N(\theta)}{\partial \theta} \frac{\partial \mathcal{L}_N(\theta)}{\partial \theta'}\Bigg|_{\theta_0}\right)=-E_f\left(\frac{\partial^2\mathcal{L}_N(\theta))}{\partial \theta \partial \theta'}\Bigg|_{\theta_0}\right)$$`
- Esta relación se conoce como la **igualdad de la matriz de información** e implica que podemos obtener la matriz de información a partir del score o del hesiano

---

# Igualdad de la matriz de información

- Recordemos que habíamos definido cuando hablamos de estimadores M, de forma general:

  - `\(A_0=p\lim \left(\frac{\partial^2Q_N(\theta)}{\partial\theta\partial\theta'} \Bigg|_{\theta_0}\right)\)`
  
  - `\(B_0=p\lim\left(N\frac{\partial Q_N(\theta)}{\partial \theta}\frac{\partial Q_N{\theta}}{\partial \theta'}\Bigg|_{\theta_0} \right)\)`
  
  
--

- La igualdad de la matriz de información implica que `\(-A_0=B_0\)`


- Por lo tanto, la varianza asintótica se simplifica a:

`$$A_0^{-1}B_0A_0^{-1}=-A_0^{-1}=B_0^{-1}$$`
--

- Las condiciones de regularidad hacen que los resultados asintóticos de los estimadores M se simplifiquen en el caso de MV

---

# Distribución del estimador de MV

- **Distribución del estimador de MV** (Proposición 5.5 en CT)

- Supongamos:

  1. El pgd es la densidad condicional `\(f(y_i|x_i,\theta)\)` usada para definir la función de verosimilitud
  
  1. La función de densidad `\(f(\cdot)\)` satisface `\(f(y,\theta^{(1)})=f(y,\theta^{(2)})\)` si y solo si `\(\theta^{(1)}=\theta^{(2)}\)`
  
  1. La matriz `\(A_0=p\lim\frac{1}{N}\frac{\partial^2\mathcal{L}_N(\theta)}{\partial \theta \partial\theta'}\Bigg|_{\theta_0}\)` existe y es no singular
  
  1. El orden de la diferenciación e integración de la función de log verosimilitud puede ser invertido
  
- Entonces el estimador de MV, definido como la solución a las condiciones de primer orden, es consistente para `\(\theta_0\)` y

`$$\sqrt{N}(\hat{\theta}_{MV}-\theta_0)\xrightarrow{d}\mathcal{N}(0,-A_0^{-1})$$`
---

# Distribución del estimador de MV

- La condición clave es 1, es decir, que el modelo está correctamente especificado

- La condición 2 es técnica e implica identificación

- La condición 3 es parecida a lo que asumiamos en MCO para poder aplicar una LGN al promedio `\(N^{-1}X'X\)`

- La mayoría de nuestras aplicaciones satisfacen el supuesto 4. Este supuesto excluye a las distribuciones cuyos rangos depende de `\(\theta\)`, como la uniforme


---

# Distribución asintótica de estimadores M

- Estos resultados implican que la distribución asintótica del estimador de MV es:

`$$\hat{\theta}_{MV}\stackrel{a}{\sim}\mathcal{N}\left(\theta,-\left( E\left(\frac{\partial^2\mathcal{L}_N(\theta)}{\partial\theta\partial\theta'}\right)\right)^{-1}\right)$$`
--

- La varianza del estimador de MV es la **cota inferior de Cramer-Rao**, o la cota inferior de la varianza de los estimadores insesgados en muestras pequeñas

- Para muestras grandes,es la cota para la matriz de varianzas de estimadores consistentes asintóticamente normales

- En pocas palabras, el estimador de MV tiene la propiedad de tener la menor varianza de entre los estimadores consistentes

- El supuesto clave para lograr esto es la correcta especificación del modelo

---

#Estimación de la matriz de varianzas de MV

- El resultado de la igualdad de la matriz de información simplifica la estimación de la matriz de varianzas del estimador de máxima verosimilitud

- Asintóticamente, la matriz `\(A_0^{-1}B_0=A_0^{-1}\)`, `\(-A_0^{-1}\)` y `\(B_0^{-1}\)` son equivalentes

--

- Dos matrices de varianza empleadas, y que son asintóticamente equivalentes son:

  - El estimador hessiano: `\(\hat{A}_H=\frac{\partial^2\mathcal{L}_N(\theta)}{\partial\theta\partial\theta'}\Bigg|_{\hat{\theta}}=H(\theta)\big|_{\hat{\theta}}\)`

  - El estimador de producto exterior del score o BHHH (Bernd, Hall, Hall &amp; Hausman, 1974): `\(\hat{A}_{BHHH}=\frac{1}{N}\sum_i\frac{\partial \mathcal{L}_N(\theta)}{\partial \theta}\big|_{\hat{\theta}}\frac{\partial \mathcal{L}_N(\theta)}{\partial \theta'}\big|_{\hat{\theta}}=\frac{1}{N}\sum_i s_i(\hat{\theta})s_i(\hat{\theta})'\)`

---

# Ejemplo: retomamos nuestro ejemplo Poisson


- En nuestro ejemplo poisson, el score es: 
`$$s_i(\beta)=(y_i-exp(x_i'\beta))x_i$$`
- Por lo que la segunda derivada es:

`$$\frac{\partial s_i(\beta)}{\partial \beta \partial \beta}=-exp(x_i'\beta)x_ix_i'$$`
--

- Por tanto, un estimador de la varianza del estimador de MV será:

`$$\hat{V}(\hat{\beta})=\left(\sum_iexp(x_i'\hat{\beta})x_ix_i'\right)^{-1}$$`

---

class: inverse, middle, center

# Mínimos cuadrados no lineales

---

# Estimador de mínimos cuadrados no lineales

- La generalización de los métodos de estimación de mínimos cuadrados es el caso en que `\(E(y|x)=g(x,\beta)\)`

- El estimador de MCNL `\(\hat{\beta}_{MCNL}\)` minimiza la suma de residuales cuadráticos `\(\sum_i (y_i-g(x_i,\beta))^2\)`, o maximiza:

`$$Q_N(\beta)=-\frac{1}{2N} \sum_i (y_i-g(x_i,\beta))^2$$`

- Las condiciones de primer orden son:
`$$\frac{\partial Q_N(\beta)}{\partial \beta}=\frac{1}{N}\sum_i\frac{\partial g_i(x_i,\beta)}{\partial \beta}(y_i-g_i(x_i,\beta))=0$$`

- Las condiciones de primer orden impican que el residual `\((y_i-g_i(x_i,\beta))\)` es ortogonal a `\(\frac{\partial g_i(x_i,\beta)}{\partial \beta}\)` (en el caso lineal, los residuales son ortogonales a las `\(x\)`)

- De nuevo, no existe una solución cerrada para `\(\hat{\beta}_{MCNL}\)` y se debe calcular usando métodos iterativos


---

# Distribución del estmador de MCNL

- Seguimos un procedimiento similar al usado en el caso de MV

- Realizamos una aproximación exacta de primer orden a las condiciones de primer orden alrededor de `\(\beta_0\)` y ordenamos términos para obtener:

`$$\sqrt{N}(\hat{\beta}_{MCNL}-\beta_0)=-\left(-\frac{1}{N}\sum_i\frac{\partial g_i}{\partial \beta} \frac{\partial g_i}{\partial \beta'}+\sum_i\frac{1}{N}\frac{\partial^2g_i}{\partial \beta\partial\beta'}(y_i-g_i)\Bigg|_{\beta^+}\right)^{-1}\left(\frac{1}{\sqrt{N}}\sum_i\frac{\partial g_i}{\partial \beta}u_i\Bigg|_{\beta_{0}}\right)$$`
- Recordemos que `\(\beta^+\)` es un valor entre `\(\hat{\beta}_{MCNL}\)` y `\(\beta_0\)`

--

- Noten que dado que `\(E(u|x)=0\)`, el término `\(\sum_i\frac{1}{N}\frac{\partial^2g_i}{\partial \beta\partial\beta'}(y_i-g_i)=0\)`, entonces nos concentramos en:

`$$\sqrt{N}(\hat{\beta}_{MCNL}-\beta_0)=\left(\frac{1}{N}\sum_i\frac{\partial g_i}{\partial \beta} \frac{\partial g_i}{\partial \beta'}\Bigg|_{\beta^{+}}\right)^{-1}\left(\frac{1}{\sqrt{N}}\sum_i\frac{\partial g_i}{\partial \beta}u_i\Bigg|_{\beta_{0}}\right)$$`

---

# Distribución del estmador de MCNL

- **Distribución del estimador de MCNL** (Proposición 5.6 en CT)

- Supongamos que:

  1. El modelo es `\(y_i=g(x_i,\beta)+u_i\)`
  1. En el pgd, `\(E(u_i|x_i)=0\)` y `\(E(uu'|X)=\Omega_0\)`, con `\(\Omega_{0,ij}=\sigma_{ij}\)`
  1. La función `\(g(\cdot)\)` satisface `\(g(x,\beta^{(1)})=g(x,\beta^{(2)})\)` si y solo si `\(\beta^{(1)}=\beta^{(2)}\)`
  1. La matriz `\(A_0\)` existe y es positiva definida
  1. `\(N^{-1/2}\frac{\partial g_i}{\partial \beta} u_i|_{\beta0}\xrightarrow{d} \mathcal{N}(0,B_0)\)`

--

- Entonces el estimador de MCNL, definido como una solución de las condiciones de primer orden es consistente para `\(\beta_0\)` y

`$$\sqrt{N}(\hat{\beta}_{MCNL}-\beta_0)\xrightarrow{d}\mathcal{N}(0,A_0^{-1}B_0A_0^{-1})$$`
- Donde `\(A_0=p\lim \frac{1}{N}\frac{\partial g_i}{\partial \beta}\frac{\partial g_i}{\partial \beta'}\Big|_{\beta_0}\)` y `\(B_0=p\lim\frac{1}{N}\sum_i\sum_j\sigma_{ij} \frac{\partial g_i}{\partial \beta} \frac{\partial g_j}{\partial \beta'}\Big|_{\beta_0}\)`

---

# Distribución del estmador de MCNL

- De nuevo, la condición 1 indica que el modelo está bien especificado

- La condición 3 es la cuestión técnica que *identifica* el parámetro en los datos

- La condición 4 nos permite aplicar una LGN al primer término

- La condición 5 permite usar un TLC 

--

- Noten que las matrices `\(A_0\)` y `\(B_0\)` son casi idénticas al caso de MCO, excepto que `\(x_i\)` es sustituida aquí por `\(\frac{\partial g_i}{\partial \beta}\Big|_{\beta_0}\)`

--

- La distribución asintótica del estimador de MCNL es:

`$$\hat{\beta}_{MCNL}\stackrel{a}{\sim}\mathcal{N}\left(\beta,(D'D)^{-1}D'\Omega_0 D(D'D)^{-1}\right)$$`
donde `\(D=\frac{\partial \mathbf{g}}{\partial \beta'}\Big|_{\beta_0}\)` es una matriz de derivadas cuya `\(i\)`ésima fila es `\(\partial g_i/\partial\beta'\big|_{\beta_0}\)`


---

# Caso homocedástico

- Un caso especial es cuando `\(\Omega_0=\sigma_0^2I\)`, por lo que `\(B_0\)` se simplifica a `\(B_0=\sigma_0^2A_0\)` y la varianza del estimador de MCNL se simplifica a:

`$$V(\hat{\beta}_{MCNL})=\sigma_0^2A_0^{-1}$$`
---

# Estimación de la matriz de varianzas de MCNL

- Consideremos el caso más general con posible heterocedasticidad de forma desconocida, por lo que nuestra tarea es estimar `\(A_0^{-1}B_0A_0^{-1}\)`

- Para `\(A_0\)`, un estimador consistente es:

`$$\hat{A}=\frac{1}{N}\sum_i\frac{\partial g_i}{\partial \beta}\Bigg|_{\hat{\beta}}\frac{\partial g_i}{\partial \beta}\Bigg|_{\hat{\beta'}}$$`
--

- Bajo independencia, `\(B_0\)` se simplifica a `\(B_0=p\lim\frac{1}{N}\sum_i\sigma_{i}^2 \frac{\partial g_i}{\partial \beta} \frac{\partial g_i}{\partial \beta'}\Big|_{\beta_0}\)`

- Como en el caso de MCO, White (1980) porpone un estimador para `\(B_0\)` (más no para `\(\sigma_i^2\)`):

`$$\hat{B}_0=\frac{1}{N}\sum_i\hat{u}_i^2\frac{\partial g_i}{\partial \beta} \frac{\partial g_i}{\partial \beta'}\Bigg|_{\hat{\beta}}=\frac{1}{N}\frac{\partial \mathbf{g}'}{\partial \beta}\Bigg|_{\hat{\beta}}\hat{\Omega}\frac{\partial \mathbf{g}}{\partial \beta}\Bigg|_{\hat{\beta}}$$`
con `\(\hat{u}_i=y_i-g(x_i,\hat{\beta})\)`

---

# Estimación de la matriz de varianzas de MCNL

- El estimador de la varianza asintótica consistente bajo heterocedasticidad del estimador de MVNL es:

`$$\hat{V}(\hat{\beta}_{MCNL})=(\hat{D}'\hat{D})^{-1}\hat{D}'\hat{\Omega}\hat{D}(\hat{D}'\hat{D})^{-1}$$`
con `\(\hat{D}=\frac{\partial \mathbf{g}}{\partial \beta'}\Big|_{\hat{\beta}}\)`


---

# Próxima sesión

- Hablaré sobre pruebas de hipótesis en general

  - CT, Capítulo 7
  
- De nuevo, será muy útil la teoría asintótica para hacer afirmaciones sobre la distribución de estadísticos de prueba

- El jueves abordaremos un caso especial que se estima por MV: variable dependiente binaria

  - CT, Capítulo 14 (secciones 14.1 - 14.4)

---

class: center, middle

Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script src="libs/cols_macro.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": null,
"scroll": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
