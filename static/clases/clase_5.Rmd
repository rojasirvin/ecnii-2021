---
title: "Pruebas de hipótesis"
author: "Irvin Rojas"
institute: "CIDE"
date: "31 de agosto de 2021"
mathspec: true
output:
  xaringan::moon_reader:
    seal: false
    chakra: "https://remarkjs.com/downloads/remark-latest.min.js"
    lib_dir: libs
    nature:
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["middle", "center"]
      ratio: "16:9"
      beforeInit: ["https://platform.twitter.com/widgets.js", "libs/cols_macro.js"]
      navigation:
      scroll: false
    css: [default, "libs/cide.css", metropolis-fonts, "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css", "https://use.fontawesome.com/releases/v5.7.2/css/all.css", "https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"]
include-before:
- '\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{s}}}{~}}'

---
class: title-slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.path = "figures/")
library(tidyverse)
library(magick)
library(reticulate)

xfun::pkg_load2(c('base64enc', 'htmltools', 'mime'))
```

.title[
# Clase 5. Pruebas de hipótesis
]
.subtitle[
## Econometría II
]
.author[
### Irvin Rojas <br> [rojasirvin.com](https://www.rojasirvin.com/) <br> [<i class="fab fa-github"></i>](https://github.com/rojasirvin) [<i class="fab fa-twitter"></i>](https://twitter.com/RojasIrvin) [<i class="ai ai-google-scholar"></i>](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&hl=en)
]

.affiliation[
### Centro de Investigación y Docencia Económicas <br> División de Economía
]

---
# Agenda
  
1. Estudiaremos los tests más usados para probar restricciones no lineales

1. Elaboraremos brevemente sobre sus propiedades asintóticas

1. Daremos algunos ejemplo de tests usados para probar algunas hipótesis con las que comúnmente nos topamos en el trabajo aplicado


---

class: inverse, middle, center

# Test de Wald

---

# Test de Wald

- Para un test de Wald se estima un modelo no restringido y se busca determinar qué tan probable es observar un estadístico bajo la hipótesis nula

- Es uno de los más usados en pruebas de hipótesis en econometría

- En Econometría I seguramente vieron la versión lineal de este test (quizás sin el nombre) cuando querían probar restricciones lineales en un modelo lineal

--

- En un modelo lineal con $y=X'\beta+u$ y con $K=4$, queremos probar la hipótesis de que $\beta_1=1$ y $\beta_2-\beta_3=2$

- $h$ es el número de hipótesis a probar (en este caso 2)

- $R$ es una matriz de $h$ x $K$

- Tenemos una hipótesis nula y una alternativa:

$$\begin{aligned}H_0: R\beta_0-r=0 \\ H_a:R\beta_0-r\neq 0 \end{aligned}$$
---

# Test de Wald

- En este caso, la matriz $R$ tiene la forma:

$$R=\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & -1 &  0  \\
\end{pmatrix}$$

- Mientras que $r=\begin{pmatrix} 1 \\ 2 \\ \end{pmatrix}$


--

- El test de Wald de $R\beta_0-r=0$ se basa en qué tan cercano a cero es el análogo muestral $R\hat{\beta}-r=0$, donde $\hat{\beta}$ es el estimador de MCO

--

- Supongamos que $u\sim\mathcal{N}(0,\sigma^2I)$, por lo que $\hat{\beta}\sim\mathcal{N}(\beta_0,\sigma^2_0(X'X)^{-1})$

- Entonces, $R\hat{\beta}-r\sim\mathcal{N}(0,\sigma^2_0R(X'X)^{-1}R')$


---

# Test de Wald

- Podemos entonces construir la forma cuadrática $W_1$:

$$W_1=(R\hat{\beta}-r)'\left(\sigma_o^2R(X'X)^{-1}R'\right)^{-1}(R\hat{\beta}-r)$$
- Que se puede mostrar que tiene una distribución $\chi^2(h)$

--

- En la práctica, no observamos $W_1$, así que usamos la versión con la varianza estimada:

$$W_2=(R\hat{\beta}-r)'\left(\hat{s}^2R(X'X)^{-1}R'\right)^{-1}(R\hat{\beta}-r)$$
- $W_2$ tiene una distribución asintítica $\chi^2(h)$ bajo la $H_0$


- Podemos calcular $W_2$ y la probabilidad de que dicho valor ocurra en una distribución $\chi^2(h)$

- Si la probabilidad es menor al nivel de significancia elegido (típicamente 5% en economía), se rechaza la $H_0$

--

- Problema: test muy restrictivo pues depende del supueto de homocedasticidad

---

class: inverse, middle, center


# Hipótesis no lineales

---

# Hipótesis no lineales


- Consideremos $h$ restricciones, posiblemente no lineales en los parámetros

- El vector de parámetros es de dimensión $q\times 1$ con $h\leq q$

--

- Queremos probar:

$$\begin{aligned}H_0: h(\theta_0)=0 \\ H_a: h(\theta_0)\neq 0 \end{aligned}$$

---

# Test de Wald no lineal

- El estadístico de Wald es:

$$W_{NL}=\hat{h}'(\hat{R}\hat{V}(\hat{\theta})\hat{R}')\hat{h}$$

con $\hat{h}=h(\hat{\theta})$ y con $\hat{R}=\frac{\partial h(\hat{\theta})}{\partial \theta'}\Bigg|_{\hat{\theta}}$

- $W_{NL}$ se distribuye asintóticamente como $\chi^2(h)$ bajo la $H_0$

- Rechazamos $H_0$ en favor de $Ha$ a un nivel de significancia $\alpha$ si $W_{NL}>\chi^2_{\alpha}(h)$

- O, equivalentemente, rechazamos $H_0$ a un nivel $\alpha$ si el valor $p$, es decir $P(\chi^2(h)>W)$ es menor que $\alpha$

---

# Derivación del estadístico de Wald no lineal


- Consideremos la restricción $h(\hat{\theta})$

- Una expansión de Taylor alrededor de $\theta_0$ resulta en:

$$h(\hat{\theta})=h(\theta_0)+\frac{\partial h(\theta)}{\partial \theta'}\Bigg|_{\theta^+}(\hat{\theta}-\theta_0)$$
- Como ya hemos hecho antes, podemos reescalar y resolver:

$$\sqrt{N}(h(\hat{\theta})-h(\theta_0))=R(\theta^+)\sqrt{N}(\hat{\theta}-\theta_0)$$
donde, en adelante, $R(\theta)=\frac{\partial h(\theta)}{\partial \theta'}$

- ¿Cuál es la distribución del lado derecho?

---

# Derivación del estadístico de Wald no lineal

- El lado derecho es $R(\theta^+)\sqrt{N}(\hat{\theta}-\theta_0)$

- Partimos del hecho de que el estimador $\hat{\theta}$ es consistente y que $\sqrt{N}(\hat{\theta}-\theta_0)\xrightarrow{d}\mathcal{N}(\mathbf{0},C_0)$

- Por su parte $R(\theta^+)\xrightarrow{p} R(\theta_0)$ (la expansión de primer orden implica esto)

- Entonces por la regla del límite del producto normal:

$$R(\theta^+)\sqrt{N}(\hat{\theta}-\theta_0)\xrightarrow{d}\mathcal{N}(\mathbf{0},R_0C_0R_0')$$
---

# Derivación del estadístico de Wald no lineal

- Por tanto:


$$\sqrt{N}(h(\hat{\theta})-h(\theta_0))\xrightarrow{d}\mathcal{N}(0,R_0C_0R_0')$$
con $C_0=V(\hat{\theta})$

- Y bajo $H_0$:


$$\sqrt{N}h(\hat{\theta})\xrightarrow{d}\mathcal{N}(0,R_0C_0R_0')$$

---

# Derivación del estadístico de Wald no lineal

- Recordemos que si $z\sim\mathcal{N}(0,\Omega)$, entonces $z'\Omega^{-1}z\sim\chi^2(dim(\Omega))$

- Entonces, bajo la $H_0$:

$$N h(\hat{\theta})'\left(R_0C_0R_0'\right)^{-1}h(\hat{\theta}) \xrightarrow{d}\chi^2(h)$$
- El estadístico $W_{NL}$ se obtiene reemplazando $R_0$ y $C_0$ por estimadores consistentes:

$$W=N\hat{h}'(\hat{R}\hat{C}\hat{R}')^{-1}\hat{h}$$
con $\hat{h}=h(\hat{\theta})$ y $\hat{R}=\frac{\partial h(\theta)}{\partial \theta'}\Bigg|_{\hat{\theta}}$

- Y sustituyendo un estimador consistente de la varianza de $\hat{\theta}$, $\hat{V}(\hat{\theta})=N^{-1}\hat{C}$, obtenemos el resultado dado en la definición del estadístico de Wald, $W_{NL}=\hat{h}'(\hat{R}\hat{V}(\hat{\theta})\hat{R}')\hat{h}$

---

# Versiones del test de Wald

**1. Estadístico F **

- Sabemos que $$F=\frac{W}{h}\xrightarrow{d}\mathcal{F}(h,N-q)$$

- En muestras grandes, deberíamos obtener el mismo valor $p$ que al usar el estadístico $W_{NL}$

- En modelos no lineales es más común usar $W_{NL}$, mientras que $F$ es preferida en muestras pequeñas


---

# Versiones del test de Wald

**2. Estadístico $t$ **

- Permite probar una restricción de exclusión

- Una normal estándar al cuadrado es una chi cuadrada con un grado de libertad, entonces

$$W_z=\frac{\hat{h}}{\sqrt{\hat{r}N^{-1}\hat{C}\hat{r}'}}$$

donde $\hat{r}=\frac{\partial h(\theta)}{\partial \theta'}\Bigg|_{\hat{\theta}}$

- $W_z$ es entonces asintóticamente normal bajo la $H_0$

- De forma equivalente, $W_z$ es asintóticamente una $t$ con $N-q$ grados de libertad, pues una $t$ converge a una normal estándar con $N\to\infty$

---

class: inverse, middle, center


# Usos del test de Wald

---

# Test de significancia

- Un test de significancia se usa para probar si $\theta_j$ es distinto de cero

- Entonces $h(\theta)=\theta_j$

- El vector $r(\theta)=\frac{\partial h}{\partial \theta'}$ es un vector de ceros, excepto en la $j$ésima entrada que toma el valor de 1

--

- En este caso, el estadístico de Wald queda como:

$$W_z=\frac{\hat{\theta}_j}{se(\hat{\theta}_j)}$$
- Un objeto familiar para ustedes, comúnmente llamdo estadístico $t$, aunque estrictamente, se distribuye asintóticamente como una normal (de hecho, normal estándar, de ahí su nombre $z$)

---

# Restricciones de exclusión

- Queremos probar que los últimos $h$ componentes de $\theta$ son 0

- Partimos $\theta$ como $\theta=(\theta_1'\;\theta_2')'$

- La restricción es: $h(\theta)=\theta_2=0$

- En este caso, $R(\theta{})=\frac{\partial h(\theta)}{\partial \theta'}=\left(\frac{\partial \theta_2}{\partial \theta_1'} \quad \frac{\partial \theta_2}{\partial \theta_2'}\right)=\left(\mathbf{0} \quad \mathbf{I}_h\right)$

--

- Y entonces el estadístico de Wald es:

$$W=\hat{\theta}_2'(N^{-1}\hat{C}_{22})^{-1}\hat{\theta}_2$$

donde $N^{-1}\hat{C}_{22}=\hat{V}(\hat{\beta}_2)$

- $W$ se distribuye $\chi^2(h)$ bajo la $H_0$

- Este test es la generalización de las pruebas $t$ y $F$ que ustedes han usado ya

---

# Tests de restricciones no lineales

- Consideren que queremos probar la $H_0$ de que $\theta_1 =\theta_2$, entonces:

$$H_0:\;h(\theta)=\frac{\theta_1}{\theta_2}-1=0$$
--

- En este caso, $R(\theta)=\left(\frac{\partial h(\theta)}{\partial \theta_1}\;\frac{\partial h(\theta)}{\partial \theta_2}\; 0\right)=\left(\frac{1}{\hat{\theta}_2}\;-\frac{\hat{\theta}_1}{\hat{\theta}_2^2}\; \mathbf{0}\right)$

- Noten que en este caso solo hay una hipótesis, por lo que $W$ es un escalar:

$$W=N\left(\frac{\hat{\theta_1}}{\hat{\theta_2}}-1\right)^2\left(\left(\frac{1}{\hat{\theta}_2}\;-\frac{\hat{\theta}_1}{\hat{\theta}_2^2}\; \mathbf{0}\right)\begin{pmatrix}\hat{c}_{11} & \hat{c}_{12} & \cdots  \\
\hat{c}_{21} & \hat{c}_{22} & \cdots  \\
\vdots & \vdots & \ddots \\
\end{pmatrix}
\left(\frac{1}{\hat{\theta}_2}\;-\frac{\hat{\theta}_1}{\hat{\theta}_2^2}\; \mathbf{0}\right)'\right)^{-1}$$

- $W$ se distribuye asintóticamente $\chi^2(1)$ (por lo que $\sqrt{W}$ es $\mathcal{N}(0,1)$)

---

class: inverse, middle, center

# Método delta

---

# Método delta

- El método que usamos para derivar el test de Wald se usando aproximaciones de Taylor se conoce como **método delta** pues involucra las derivadas de $h(\theta)$

- Este método es más general y puede ser utilizado para obtener la distribución de una combinación no lineal de parámetros y hacer inferencia

- Por ejemplo, quisiéramos obtener $\frac{\hat{\theta}_1}{\hat{\theta}_2}$ y dar un intervalo de confianza para este cociente

---

# Método delta

- Supongamos que estimamos un modelo donde parametrizamos $E(y|x)=\exp\{x'\beta\}$ y estimamos $\hat{\beta}$

- Estamos interesados en un intervalo de confianza para $E(y|x_p)$, es decir, la media condicional para un valor dado $x_p$

- En este caso:

$$h(\beta)=\exp\{x_p'\beta\}$$

$$R(\beta)=\partial h(\beta)/\partial \beta'=\exp\{x_p'\beta\}x_p$$

- Y el método delta nos dice la forma del error estándar:

$$se(\exp\{x_p'\hat{\beta}\})=\sqrt{\exp\{x_p'\hat{\beta}\}^2x_p'N^{-1}\hat{C}x_P}=\exp\{x_p'\hat{\beta}\}\sqrt{x_p'N^{-1}\hat{C}x_P}$$
- $\hat{C}$ es un estimador consistente de la matriz de varianzas de $\sqrt{N}(\hat{\beta}-\beta_0)$

- Y formamos un intervalo de confianza de 95% con $\exp\{x_p'\hat{\beta}\}\pm 1.96 se(\exp\{x_p'\hat{\beta}\})$ 


---

class: inverse, middle, center

# Tests basados en la verosimilitud

---

# Tests basados en la verosimilitud

- Se asume que la función de verosimilitud es conocida

- Queremos probar la hipótesis $h(\theta_0)=0$

- Para el test de razón de verosimilitud (LR) y para el test de multiplicador de Lagrange (LM), se requiere llevar a cabo la estimación con y sin restricciones:
  
  - $\hat{\theta}_u$: estimador no restringido
  - $\tilde{\theta}_r$: estimador restringido
  
--

- El estimador restringido $\tilde{\theta}_r$ maximiza el lagrangiano $\mathcal{L}(\theta)-\lambda'h(\theta)$, donde $\lambda$ es un vector de multiplicadores de Lagrange de dimension $h\times 1$

---

# Ejemplo: test de exclusión

- Supongamos $\theta=(\theta_1',\theta_2')'$ y queremos probar la hipótesis de exclusión de $h(\theta)=\theta_2=0$

- Entonces, $\tilde{\theta}_r=(\tilde{\theta}_{1r}',0')$

- $\tilde{\theta}_{1r}$ se obtiene maximizando con respecto a $\theta_1$ la log verosimilitud restringida $\mathcal{L}(\theta_1,0)$

---

# Test de razón de verosimilitud (LR)

- La intuición es que si la $H_0$ es verdadera, los máximos de la verosimilitud restringida y de la no restringida deberían ser los mismos

- Entonces, se usa una función de la diferencia entre $\mathcal{L}(\tilde{\theta}_r)$ y $\mathcal{L}(\hat{\theta}_u)$

--

- Definimos:

$$LR=-2(\mathcal{L}(\tilde{\theta}_r)-\mathcal{L}(\hat{\theta}_u))$$

- Se puede mostrar que bajo $H_0$ este test tiene una distribución asintótica $\chi^2(h)$

$$LR\sim\chi^2(h)$$

---

# Test de multiplicador de Lagrange

- Sabemos que en el máximo, $\partial\frac{\mathcal{L}(\theta)}{\partial \theta}\Bigg|_{\hat{\theta}_u}=0$

- Si la $H_0$ es verdadera, entonces debe suceder que $\frac{\partial\mathcal{L}(\theta)}{\partial \theta}\Bigg|_{\tilde{\theta}_r}\approx 0$

- Noten que esto implica diferencias en el *score* evaluado en distintos $\theta$, por lo que a este test también se le conoce como **test de scores**


--

- El test se define como

$$LM=-N^{-1}\frac{\partial \mathcal{L}}{\partial\theta'}\Bigg|_{\tilde{\theta}_r}\tilde{A}^{-1}\frac{\partial \mathcal{L}}{\partial\theta'}\Bigg|_{\tilde{\theta}_r}$$
- Recordemos que por la igualdad de la matriz de información $V(\hat{\theta})=p\lim N^{-1}\frac{\partial^2\mathcal{L}}{\partial \theta \partial \theta'}\Bigg|_{\theta_0}$

- Por tanto, $\tilde{A}$ es un estimador consistente de $A_0$, pero evaluado en $\hat{\theta}_r$

---

# Test de multiplicador de Lagrange

- La intuición del nombre de este test viene de que el problema equivale a resolver el problema de optimización restringido de maximizar $\mathcal{L}(\theta)-\lambda'h(\theta)$


- La prueba entonces consiste en ver qué tan cercano es el multiplicador de Lagrange de este problema

--

- Para su implementación se obtiene una expresión para el score y se evalúa en $\tilde{\theta}_r$, de ahí que algunos consideren más apropiado el nombre de test de score

- De nuevo, este test tiene una distribución asintótica $\chi^2(h)$

$$LM\sim\chi^2(h)$$

---

# Regla de decisión

- Los tests basados en verosimilitud que hemos visto convergen en distribución a una $\chi^2(h)$ bajo la $H_0$

- La regla de decisión se puede escribir de dos formas alternativas

  - Rechazar $H_0$ al nivel de significancia elegido $\alpha$ si el estadístico es mayor que $\chi_{\alpha}(h)$
  
  - Rechazar $H_0$ a un nivel $\alpha$ si $p\leq\alpha$, donde $p=P(\chi_{\alpha}(h))>t$, donde $p$ se conoce como *valor* $p$ y $t$ es el estadístico calculado


---

class: inverse, middle, center

# Valores $p$

---

# El valor $p$

- La otra cara de la moneda de los estadísticos de prueba es el valor $p$

- El valor $p$ es la probabilidad de observar un valor mayor que el estadístico cuando la $H_0$ es verdadera

- Un valor $p$ muy pequeño indica que es muy poco probable observar el estadístico de prueba bajo la $H_0$, por lo que hay evidencia para rechazar la $H_0$

- Otra forma de intepretar el valor $p$ es la probabilidad de que se observen tests iguales o más grandes a los observados debido al error muestral (*por suerte*)

---

# Valores $p$

- Consideremos el estadísto $t$ de una prueba de significancia estadística, donde $H_0: \beta_0=0$

- Supongamos $\hat{\beta}=100$ con un valor $p$ de 0.07

- Entonces, incluso si $\beta_0=0$, todavía sería posible ver $\hat{\beta}=100$ o más en el 7% de los casos debido al error muestral

- En este breve texto de [Krzywinski & Altman (2013)](https://www.nature.com/articles/nmeth.2698) pueden leer algunos otros detalles sobre el valor $p$

```{r, out.width="80%",fig.cap='Fuente: Krzywinski & Altman (2013)',fig.align='center'}
knitr::include_graphics("figures/pvalue.jpg")
```

---

# Valores $p$

- ¿Qué tanto toleramos que nuestros resultados puedan ser *por suerte*?

- Fijamos un nivel de significancia $\alpha$, definido como la probabilidad de rechazar la $H_0$ dado que esta es verdadera

- Es decir, la probabilidad de cometer el **error tipo 1** o **falso positivo**

- En economía usamos frecuentemente los valores $\alpha$ de 0.10, 0.05 y 0.01 como niveles de significancia más comúnes

---

# Próxima sesión

- Comenzaré con los modelos de variable binaria

  - CT Capítulo 16

---

class: center, middle

Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**