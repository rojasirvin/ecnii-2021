---
title: "Variable dependiente binaria"
author: "Irvin Rojas"
institute: "CIDE"
date: "2 de septimebre de 2021"
mathspec: true
output:
  xaringan::moon_reader:
    seal: false
    chakra: "https://remarkjs.com/downloads/remark-latest.min.js"
    lib_dir: libs
    nature:
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["middle", "center"]
      ratio: "16:9"
      beforeInit: ["https://platform.twitter.com/widgets.js", "libs/cols_macro.js"]
      navigation:
      scroll: false
    css: [default, "libs/cide.css", metropolis-fonts, "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css", "https://use.fontawesome.com/releases/v5.7.2/css/all.css", "https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"]
include-before:
- '\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{s}}}{~}}'

---
class: title-slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.path = "figures/")

library(tidyverse)
library(magick)
library(reticulate)
library(pacman)
library(janitor)
library(sandwich)
library(modelsummary)
p_load(tidyverse, foreign, reshape2, psych, qwraps2, forcats, readxl, 
       broom, lmtest, margins, plm, rdrobust, multiwayvcov,
       wesanderson, sandwich, stargazer,
       readstata13, pscore, optmatch, kdensity, MatchIt, bootstrap, matlib, dplyr)

xfun::pkg_load2(c('base64enc', 'htmltools', 'mime'))
```

```{css, echo = FALSE}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 60% !important;
}
```

.title[
# Clase 6. Variable dependiente binaria
]
.subtitle[
## Econometría II
]
.author[
### Irvin Rojas <br> [rojasirvin.com](https://www.rojasirvin.com/) <br> [<i class="fab fa-github"></i>](https://github.com/rojasirvin) [<i class="fab fa-twitter"></i>](https://twitter.com/RojasIrvin) [<i class="ai ai-google-scholar"></i>](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&hl=en)
]

.affiliation[
### Centro de Investigación y Docencia Económicas <br> División de Economía
]

---
# Agenda
  
1. Estudiaremos las propiedades generales teóricas de estimadores para modelos cuando la variable dependiente es binaria

1. Mostraremos algunos resultados generales de las densidades de la familia lineal exponencial

1. Mostraremos las propiedades asintóticas de los estimadores de MV para este tipo de modelos

---

# Introducción

- Frecuentemente nos encontramos con problemas donde la variable dependiente es categórica

  - Probabilidad de comprar o no comporar un producto
  
  - Probabilidad de escoger el producto $j$ de entre $J$ posibles alternativas
  
  - Probabilidad de tener una tarjeta $k$ de entre las varias tarjetas $K$ que tiene una jerarquía
  
--

- MCO ignora la naturaleza discreta de la variable dependiente

- Estudiaremos modelos que parametrizan la probabilidad individual de que ocurra un evento y que se estiman por MV

- Afortunadamente ya sabemos mucho de MV

---

class: inverse, middle, center

# Variable dependiente binaria

---

# Variable dependiente binaria

- $y_i$ toma el valor de 1 si el evento se realiza y 0 si no

- Los datos siguen una distribución Bernoulli con probabilidad que varía entre individuos: $p\equiv p_i$

- Especificamos una forma funcional para la probabilidad y se estima por MV

---

# Modelo general

- La variable dependiente:
$$
y_i=
\begin{cases}
1 \quad\text{con probabilidad }p \\
0 \quad\text{con probabilidad }1-p
\end{cases}
$$
- Parametrizamos $p_i$ con un vector de características $x_i$ y un vector de parámetros $\beta$:

$$p_i=F(y_i=1|x_i)=F(x_i'\beta)$$
- A $x_i'\beta$ se le conoce como *índice*, por lo que este modelo es también un modelo de un índice único (*single index model*)

--

- $F$ es una función de distribución acumulada (cdf)

- Un modelo de probabilidad lineal simplemente especifica $p_i=x_i'\beta$

---

# Probit y logit

- Un modelo probit especifica $F(\cdot)$ como una normal estándar con cdf dada por:

$$\Phi(x'\beta)=\int_{-\infty}^{\infty}\phi(z)dz$$

--

- Un modelo logit especifica a $F(\cdot)$ como una función logística:

$$\Lambda(x'\beta)=\frac{exp\{x'\beta\}}{1+exp\{x'\beta\}}$$
---

# Efectos marginales

- En un modelo lineal, $\beta_j$ tiene la interpretación directa del efecto de un cambio marginal en $x_j$ sobre $y$

- En cambio, en los modelos de probabilidad no lineal estamos interesaods en:

$$\frac{\partial P(y_i=1)|x_i)}{\partial x_{ij}}=F'(x_i'\beta)\beta_j$$

- Como $F(\cdot)$ es no lineal, los efectos marginales difieren del punto de evaluación, es decir, de $x_i'\beta$

--

- En el caso probit:

$$\frac{\partial P(y_i=1)|x_i)}{\partial x_{ij}}=\phi(x'\beta)\beta_j$$

--

- En el caso logit:

$$\frac{\partial P(y_i=1)|x_i)}{\partial x_{ij}}=\Lambda(x'\beta)(1-\Lambda(x'\beta))\beta_j$$
---

# Efectos marginales

- Dos efectos marginales que podemos calcular:

1. Promedio de efectos marginales: $$\frac{1}{N}\sum_i F'(x_i'\hat{\beta})\hat{\beta}_j$$

1. Efecto marginal evaluaado en la media de $x$: $$F'(\bar{x}'\hat{\beta})\hat{\beta}_j$$

- En el trabajo empírico es más común usar el promedio de efectos marginales

- Una crítica al efecto marginal en la media es que $\bar{x}$ puede no representar nada de los individuos en la muestra

---

# Efectos marginales

- Noten que el cociente de efectos marginales es igual al cociente de los coeficientes estimados:

$$\frac{\frac{\partial P(y_i=1)|x_i)}{\partial x_{ij}}}{\frac{\partial P(y_i=1)|x_i)}{\partial x_{ik}}}=\frac{\hat{\beta}_j}{\hat{\beta}_k}$$


---

class: inverse, middle, center

---

# Estimación

- Tenemos a la mano datos $(y_i,x_i)$ de $N$ individuos

- La función de masa de probabilidad para $y_i$ es:

$$f(y_i|x_i)=p_i^{y_i}(1-p_i)^{1-y_i},\quad\quad y_i={0,1}$$
- Recordemos que $p_i=F(x_i'\beta)$

---

# Estimación

- La log densidad será:

$$\ln f(y_i)=y_i\ln p_i + (1-y_i)\ln(1-p_i)$$
--

- Por independencia sobre $i$, la función de log verosimilitud es:

$$\mathcal{L}(\beta)=\sum_i\{y_i\ln p_i + (1-y_i)\ln(1-p_i)\}$$
--

- Sustituyendo $F$ en vez de $p_i$:


$$\mathcal{L}(\beta)=\sum_i\{y_i\ln F(x_i'\beta) + (1-y_i)\ln(1-F(x_i'\beta))\}$$
---

# Estimación

- La condición de primer orden implica que $\hat{\beta}_{MV}$ resuleve:

$$\sum_i \left(\frac{y_i-F(x_i'\beta)}{F(x_i'\beta)(1-F(x_i'\beta))}F'(x_i'\beta)x_i\right)=0$$

---

class: inverse, middle, center

# Cuasi máxima verosimilitud

---
# Cuasi máxima verosimilitud

- Hagamos aquí una breve desviación a un aspecto particular de MV y luego veremos cómo empata con la discusión sobre los modelos de probabilidad no lineal

  - Ver la Sección 5.7.3 en CT

--

- Un estimador $\hat{\theta}_{CMV}$ es aquel estimador que maximiza una función de log verosimilitud que está mal especificada

- Generalmente, si la densidad está mal especificada, el estimador de MV será inconsistente

- En casos especiales, el estimador de CMV es aún consistente

---

# Familia lineal exponencial

- Una densidad de la familia lineal exponencial puede ser expresada como
$$f(y|\mu)=\exp\{a(\mu)+b(y)+c(\mu)y\}$$
- Diferentes formas funcionales para $a$, $b$ y $c$ dan lugar a distintas densidades

- Algunas densidades comúnmente usadas que son de la familia lineal exponencial incluyen la normal (con varianza conocida), la Bernoulli, la exponencial y la Poisson (ver Tabla 5.4 en CT)

---

# Familia lineal exponencial

- Supongamos que parametrizamos la media como $\mu=g(x,\beta)$

- La función de log verosimilitud con una densidad de la familia lineal exponencial será:

$$\mathcal{L}_N(\beta)=\sum_i\left(a(g(x_i,\beta)+b(y_i)+c(g(x_i,\beta))y_i\right)$$

--

- Las condiciones de primer orden serán

$$\frac{\partial \mathcal{L}_N(\beta)}{\partial \beta}=\sum_i\frac{y_i-g(x_i\beta)}{(c'(g(x_i'\beta)))^{-1}}\frac{\partial g(x_i,\beta)}{\partial\beta}=0$$
- El estimador de CMV resuelve estas condiciones, pero no es necesario asumir que la densidad está correctamente bien especificada

- Gouriéroux, Monfort y Trognon (1984) probaron que el estimador de CMV es consistente si $E(y|x)=g(x,\beta_0)$, es decir, si la media condicional de $y$ dado $x$ está bien especificada

---

# Familia lineal exponencial

- Aun cuando la media condicional esté bien especificada, en la práctica se debe usar una matriz de varianzas de sándwich $A_0^{-1}B_0A_0^{-1}$, a menos de que la varianza condicional también esté bien especificada

- En el caso Bernoulli, la varianza condicional está bien especificada, por lo que basta usar 
$-A_0^{-1}$

--

- Hasta aquí el fin de esta nota

---

class: inverse, middle, center

# De regreso a variable dependiente binaria

---

# Consistencia

- Notemos que para datos Bernoulli se cumple que:

$$E(y)=1\times p+ 0 \times (1-p)=p$$
- Es decir, $E(y_i|x_i)=F(x_i'\beta)$, por lo que el lado derecho de las condiciones de primer orden tiene valor esperado de cero

- Es decir, el estimador de MV es consistente si la media condicional está bien especificada

--

- Esto ya lo sabíamos porque la Bernoulli es de la familia lineal exponencial

---

# Distribución asintótica

- Si la densidad está **bien especificada**, la teoría que vimos sobre MV indica que el estimador tendrá una distribución asintótica como sigue:
$$\hat{\beta}_{MV}\stackrel{a}{\sim}\mathcal{N}\left(\beta, \left(-E\left(\frac{\partial^2\mathcal{L}}{\partial\beta\partial\beta'}\right)\right)^{-1}\right)$$

- Tomando la derivada a las condiciones de primer orden y calculando el negativo del valor esperado obtenemos:

$$\hat{V}(\hat{\beta}_{MV})=\left(\sum_i\frac{1}{F(x_i'\hat{\beta})(1-F(x_i'\hat{\beta}))}F'(x_i'\hat{\beta})^2x_ix_i'\right)^{-1}$$
--

- Por nuestra discusión de la familia lineal exponencial sabemos que solo necesitamos correcta especificación de la media condicional para tener consistencia

- Pero que se recomienda usar una matriz de sándwich para estimar la varianza del estimador de MV

---

# Distribución asintótica

- En el caso Bernoulli, se puede mostrar que $A=-B$, por lo que el sándwich resulta $A^{-1}BA^{-1}=-A^{-1}$

- Solo en el caso Bernoulli, no hay ninguna ventaja al usar una matriz de varianzas robusta cuando los datos son independientes sobre $i$

- Veremos más adelante que cuando hay errores agrupados sí usaremos una matriz robusta

---

# Particularidades del modelo logit

- Una medida comúnmente usada es la razón de momios u *odds ratio*, también llamado riesgo relativo: $\frac{p}{1-p}$

- El riesgo relativo es la probabilidad de que suceda $y=1$ relativa a la probabilidad de que $y=0$

- En el caso del logit, el riesgo relativo es:

$$\frac{p}{1-p}=exp\{x'\beta\}$$

- Y el log del riesgo relativo es simplemente:

$$\ln\left(\frac{p}{1-p}\right)=x'\beta$$

- Es decir, el log del riesgo relativo o el log de la razón de momios es lineal en $x$

---

# Particularidades del modelo logit

- Noten que expresar las probabilidades como riesgo relativo tiene una interpretación usada comúnmente en bioestadística

- Si $\frac{p}{1-p}=exp\{x'\beta\}$ y $x_j$ cambia en una unidad, entonces el lado derecho se vuelve $exp\{x'\beta+\beta_j\}=exp\{x'\beta\} exp\{\beta_j\}$

- Es decir, el riesgo relativo se ha incrementado $exp\{\beta_j\}$ veces

- Supongamos que $\hat{\beta}_j=0.05$, entonces $exp\{0.05\}\approx 1.05$

--

- Es decir, el riesgo relativo se incrementa en aproximadamente 5%

---

# ¿Probit o logit?

- Empíricamente suelen desempeñarse de forma muy similar

- Como nos interesan los efectos marginales, la diferencia entre los modelos usados suele ser mínima

- El modelo logit es frecuentemente usado en bioestadística por su interpretación en términos de riesgo relativo

- El probit se puede motivar por un modelo de variable latente normal, que se liga directamente al modelo Tobit (que veremos más adelante)


---

# Ventajas del uso de modelos de probabilidad no lineal

```{r, out.width="50%",fig.cap='Fuente: Figura 14.1 en Cameron y Trivedi (2005)',fig.align='center'}
knitr::include_graphics("figures/CT_pnl.png")
```

---

class: inverse, center, middle

# Ejemplo con los datos de juguete de pesca

---

# Ejemplo: probit y logit

- Datos de Herriges & Kling (1999) usados y provistos por Cameron & Trivedi (2005)

- Exploramos los datos:
  - Las observaciones son clientes
  
  - *mode* indica la elección del lugar donde pescó
  
  - El ingreso de la persona es *income*
  
  - La tasa de pesca es *q*
  
  - El precio es *p*
  
```{r tidy=T, echo=TRUE, message=F, results='hide'}
data_fishing<-read_csv("./fishing_data.csv",
                       locale = locale(encoding = "latin1"))   %>% 
  clean_names() 
  

```

---
# Ejemplo: probit y logit

```{r tidy=T, echo=T, include=T}

head(data_fishing, n=5)

names(data_fishing)

```

---

# Ejemplo: probit y logit

- Modelo probit



```{r echo=T, include=T, message=F}
# Para logit y probit usaremos solo barco y puerto

data_binary<-data_fishing %>% 
  filter(mode==2 | mode==4) %>% 
  mutate(charter=ifelse(mode==4,1,0)) %>% 
  mutate(pratio=100*log(pcharter/ppier),
         lnrelp=log(pcharter/ppier))

mprobit <- glm(charter ~ lnrelp, family = binomial(link = "probit"), 
    data = data_binary)

```


```{r tidy=T, echo=T, include=T, message=F}
#Pedimos los coeficientes
summary(mprobit)$coef


```
---

# Ejemplo: probit y logit

- Modelo logit y de probabilidad lineal


```{r, echo=T, include=T, message=F}
mlogit <- glm(charter ~ lnrelp,
              family = binomial(link = "logit"),
              data = data_binary)

mlineal <- lm(charter ~ lnrelp,
              data=data_binary)

```
---

# Ejemplo: probit y logit

- Un paquete reciente que acabo de descubrir es *modelsummary*

.pull-left[
```{r, echo=T, include=T, message=F}
#Colección de modelos
models<-list('Logit'=mlogit, 'Probit'=mprobit, 'Lineal'=mlineal)

#cm controlará los nombres de coeficientes
cm <- c( '(Intercept)' = 'Constante', 'lnrelp' = 'ln relp')

#gm controlará las medidas de bondad de ajuste
gm <- modelsummary::gof_map
gm$omit <- TRUE
gm <- gm %>% 
  mutate(omit=ifelse(raw=="logLik",F,omit))

```
]

.pull-right[
```{r echo=T, include=T, message=F}

# Usamos msummary

msummary(models,
         statistic='statistic',
         coef_map = cm,
         gof_map = gm)

```
]
---

#Ejmeplo: probit y logit

.pull-left[
- Predicción

```{r, echo=T, include=T, message=F}

#Predicciones con cada modelo

plogit <- mlogit %>%
  predict(data=data_binary, type = "response")

pprobit <- mprobit %>%
  predict(data=data_binary, type = "response")

plineal <- mlineal %>%
  predict(data=data_binary, type = "response")

#Colecciono las variables que usaré
data_for_plot <- data.frame(lnrelp=data_binary$lnrelp,
                            plogit,
                            pprobit,
                            plineal)
```
]



.pull-right[
```{r, echo=T, include=T, message=F, fig.height=3}

#Arreglo en formato long (lo usaremos más en panel)
data_for_plot <- pivot_longer(data_for_plot,
                              cols= c("plogit","pprobit","plineal"),
                              names_to="Modelo",
                              values_to = "prob")

#Construyo la gráfica
plot_probs <- data_for_plot %>% 
  ggplot()+
  geom_line(aes(x=lnrelp,y=prob, color=Modelo))

plot_probs
```
]

---

# Próxima sesión

- Veremos modelos multinomiales

  - CT Capítulo 15

- Es la generalización del problema que estudiamos hoy: la variable $y$ puede tomar no solo dos valores, sino $J$ posibles valores correspondientes a una categoría mutuamente excluyente

---

class: center, middle

Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**