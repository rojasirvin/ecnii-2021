---
title: "Respuestas a la tarea 4"
summary: " "
weight: 2
type: book
toc: false
---

```{r setup, include=FALSE}
library(tidyverse)
library(quantreg)
library(survival)
library(survminer)  #graficar curva de supervivencia estimada con ggsurvplot()
library(SurvRegCensCov) #contiene la transformación de los resultados Weibull usando ConvertWeibull
library(plm)
library(stargazer)

xfun::pkg_load2(c('base64enc', 'htmltools', 'mime'))
```

## Instrucciones
  
Las tareas deben entregarse de manera individual, pero se recomienda ampliamente colaborar en grupos de estudio. Las tareas deberán entregarse en Teams antes de la fecha y hora señalada. No se aceptarán tareas fuera de tiempo. Por favor, **no comprima los archivos** en carpetas comprimidas. Las tareas deberán contener dos archivos:

Un primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento puede ser redactado en Word o cualquier otro software, o si lo prefiere, a mano, pero deberá estar impreso en .pdf. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.

Un segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad.

Recuerde que en Teams debe asegurarse de que los archivos se han subido correctamente.

## Fecha de entrega

Viernes 3 de diciembre a las 20:00.

## Archivos

[MunichRent.rda](/tareas/tarea_4/MunichRent.rda)

[crimen_nc.csv](/tareas/tarea_4/crimen_nc.csv)

[data_rossi.csv](/tareas/tarea_4/data_rossi.csv)


# Pregunta 1

Considere los datos en *MunichRent.rda*. Estos archivos contienen información sobre rentas en la ciudad de Munich, **rent**. Se desea modelar la renta en función de la antiguedad de los edificios en renta, controlando por el área, **area**. También se desea controlar por el tipo de baño y cocina que tiene el edificio, usando efectos fijos de las variables **bath** y **kitchen**. La variable **yearc** indica cuándo fue construido el edificio. Para leer los datos basta con ejecutar lo siguiente:

```{r eval=F,ech=T,results=F}
load("./MunichRent.rda")
```

a. [10 puntos] Estime la densidad Kernel de las rentas por metro cuadrado, **rentsqm**, usando un Kernel gaussiano. Use el estimador *plug-in* de Silverman para estimar el ancho de ventana óptimo.

    *Simplemente seguimos el procedimiento que vimos en clase para obtener $h^*$.

    ```{r eval=T, include=T, echo=T, message=F}
load("C:/Users/rojas/Dropbox/sitios_git/ECNII2020/tareas/MunichRent.rda")

rentsqm.sd <- sd(MunichRent$rentsqm)
rentsqm.iqr.adj <- IQR(MunichRent$rentsqm)/1.349
rentsqm.N <- nrow(MunichRent)
constante <- 1.3643
ajuste <- min(rentsqm.sd,rentsqm.iqr.adj)

delta <- 0.7764
h <- constante*delta*ajuste*rentsqm.N^(-0.2) 
```
    *Ahora hacemos el gráfico alimentando el valor de $h$:
    
    ```{r eval=T, include=T, echo=T, message=F}
MunichRent %>% 
  ggplot(aes(x=rentsqm)) +
  geom_histogram(aes(y=..density..), bins=20, fill="#69b3a2", color="#e9ecef", alpha=0.9)+
  geom_density(aes(x=rentsqm, color='Óptimo'), kernel="gaussian", bw=h/2, adjust=1)
    ```

a. [10 puntos] Estime por MCO la relación entre el la renta, **rent** y la antiguedad del edificio, controlando por **area** y efectos fijos de **bath** y **kitchen**. Estime la misma relación, pero con una regresión mediana. Interprete los coeficientes sobre la antiguedad en ambos casos.

    *Primero por MCO obtenemos una relación positiva entre la renta y el área y una relación negativa entre la renta y la antiguedad, como era de esperarse. Ambos coeficientes estimados son estadísticamente significativos.*

    ```{r eval=T, include=T, echo=T, message=T}
MunichRent <- MunichRent %>% 
  mutate(antiguedad=2020-yearc)

#Por MCO
summary(r.mco <- lm(rent  ~ area+antiguedad+factor(bath)+factor(kitchen),
                    data=MunichRent))
```

    *Ahora realizamos un modelo LAD*:

    ```{r eval=T, include=T, echo=T, message=T}
summary(r.q50 <- rq(rent  ~ area+antiguedad+factor(bath)+factor(kitchen),
                    data=MunichRent,
                    tau=0.5))
```
    
    *Los coeficientes estimados son de una magnitud similar a los de MCO.*

a. [10 puntos] Estime ahora una regresión cuantil para cada uno de los deciles de la distribución de la renta y represente en una gráfica los coeficientes por regresión cuantil junto con el coeficiente de MCO para las variables del área y la antiguedad. ¿Concluye que vale la pena modelar la relación de las rentas en función del área y la antiguedad usando regresión cuantil?

    *Regresión cuantil para cada decil:*

    ```{r eval=T, include=T, echo=T, message=T}
r.q1_9 <- rq(rent  ~ area+antiguedad+factor(bath)+factor(kitchen),
                    data=MunichRent,
                    tau= 1:9/10)

plot(summary(r.q1_9), parm=c("area","antiguedad"))

```


# Pregunta 2

Considere los datos en *crimen_nc.csv*. Estos son datos anuales de 1981 a 1987 sobre crimen del estado de Carolina del Norte en Estados Unidos. Se desea modelar el log de la tasa de crimen, **lcrmrte**, en función del log de la probabilidad de arresto (**lprbarr**) y del log del número de policías per cápita (**lpolpc**).

Se propone incluir como controles la probabilidad de sentencia (**lnprbconv**), la probabilidad de ir a prisión (**lprbpris**), el tiempo promedio de sentencia (**lavgsen**), la densidad de población (**ldensity**), el porcentaje de población joven (**lpctmle**), el porcentaje de la población no blanca (**lpctmin**), los salarios semanales en la construcción (**lwcon**), el transporte (**lwtuc**), el comercio (**lwtrd**), las finanzas (**lwfir**), los servicios (**lwser**), la manufactura (**lwmfg**) y los gobierno federal, estatal y local (**lwfed**, **lwsta** y **lwloc**). Además, se propone controlar por una dummy que indica una ciudad grande (**smsa**) y dummies regionales (**region**), así como efectos fijos por condado y por año.

a. [10 puntos] Estime un modelo de efectos fijos con los regresores descritos anteriormente. Interprete los coeficientes sobre el log de la probabilidad de arresto y el log del número de policías per cápita.

    *Modelo de efetos fijos. Existe una relación negativa entre los arrestos y la tasa de crimen. Sin embargo, parece haber una relación positiva entre el número de policías y la tasa de crimen.*
    
    ```{r eval=T, include=T, echo=T, message=F}

data.crime<-read_csv("./crimen_nc.csv",
                  locale = locale(encoding = "latin1")) 
    

cr.within <- plm(lcrmrte ~ lprbarr + lpolpc +
                   lprbconv + lprbpris + lavgsen +
                   ldensity + lpctymle + lpctmin + 
                   lwcon + lwtuc + lwtrd + lwfir + 
                   lwser + lwmfg + lwfed + lwsta + 
                   lwloc + region + smsa + factor(year),
                 data = data.crime,
                 model = "within",
                 index = c("county","year"))
summary(cr.within)

```


a. [10 puntos] Existe la preocupación de que la variable que indica el número de policías es endógena. Se propone usar como instrumento el porcentaje de los crímenes que son cara a cara (robos, violación, asaltos) con respecto al resto (**lmix**) pues se argumenta que estos crímenes facilitan la identificación del criminal. Se propone usar también como instrumento el ingreso per cápita por impuestos (**ltaxpc**) bajo el argumento de que los condados con preferencia por hacer valer la ley tendrán mayores ingresos para financiar la policía. Estime el modelo de efectos fijos tomando en cuenta la endogeneidad e incluyendo las dos variables propuestas como instrumentos. ¿Cómo cambian sus resultados sobre el efecto del número de policías en el crimen?

    *Noten como _plm_ funciona muy bien para modelos en panel con endogeneidad. Después de "|" incluimos un punto para indicar que conserve los regresores del modelo estructural, quitando el regresor endógeno y añadiendo los instrumentos. Al especificar la endogeneidad del número de policías los resultados son cualitativamente similares a los que se obtuvieron en la parte a.*

    ```{r eval=T, include=T, echo=T, message=T}
cr.within.iv1 <- plm(lcrmrte ~ lprbarr + lpolpc +
                       lprbconv + lprbpris + lavgsen +
                       ldensity + lpctymle + lpctmin +
                       lwcon + lwtuc + lwtrd + lwfir +
                       lwser + lwmfg + lwfed + lwsta +
                       lwloc + region + smsa + factor(year) |
                       . -lprbarr  + lmix + ltaxpc,
                 data = data.crime,
                 model = "within",
                 index = c("county","year"))
summary(cr.within.iv1)

```

a. [10 puntos] Se sospecha que la probabilidad de ser arrestado es también endógena. Use los mismos instrumentos que en la parte c. para estimar el modelo de efectos fijos con dos variables endógenas. ¿Cómo cambian sus conclusiones respecto a número de policías y la probabilidad de ser arrestado con respecto a la parte a.?

    *Ahora el modelo exactamente identificado:*

    ```{r eval=T, include=T, echo=T, message=T}
cr.within.iv2 <- plm(lcrmrte ~ lprbarr + lpolpc +
                       lprbconv + lprbpris + lavgsen +
                       ldensity + lpctymle + lpctmin +
                       lwcon + lwtuc + lwtrd + lwfir +
                       lwser + lwmfg + lwfed + lwsta +
                       lwloc + region + smsa + factor(year) |                       lmix + ltaxpc + lprbarr +
                       . -lprbarr - lpolpc + lmix + ltaxpc,
                     data = data.crime,
                     model = "within",
                     index = c("county","year"))
summary(cr.within.iv2)

```

    *Comparamos los tres modelos estimados*:

    ```{r eval=T, include=T, echo=T, message=T}
stargazer(cr.within,cr.within.iv1, cr.within.iv2, type = "text")

```

    *Estos datos son de un estudio de [Baltagi (2006)](https://onlinelibrary.wiley.com/doi/epdf/10.1002/jae.861). Pueden ver que los resultados que produjeron están en la Tabla 1 de su estudio.*

a. [5 puntos] ¿Qué se debe de asumir sobre la exogeneidad de los instrumentos para implementar los estimadores de efectos fijos de las partes b. y c.?

# Pregunta 3

Los datos en *rossi_arrestos.csv* contienen información de 432 personas que fueron arrestadas en algún momento de sus vidas y luego puestas en libertad. A estas personas se les siguió durante 52 semanas para estudiar el tiempo que transcurrió hasta que reincidieron en cometer un delito y fueron arrestadas nuevamente. **week** indica cuántas semanas pasaron desde que fueron liberados hasta ser de nuevo arrestados y la variable **arrest** indica el evento de ser arrestado. Un programa público consistió en darle ayuda financiera a un grupo seleccionado al azar e identificado por **fin**. Las variables de control para el análisis son la edad (**age**), indicadoras de raza  (**race**), la experiencia laboral (**wexp**), indicadoras de estado civil (**mar**), una indicadora de si la liberación fue por perdón (**paro**), y el número de arrestos previos (**prio**).

a. [10 puntos] Represente la función de sobrevivencia no paramétrica de Kaplan-Meier.

    ```{r eval=T, include=T, echo=T, message=T}
    data.rossi <- read_csv("./data_rossi.csv",
                  locale = locale(encoding = "latin1")) 
    
    #KM
    km <- survfit(Surv(week, arrest) ~ 1,
                     type = "kaplan-meier",
                     data=data.rossi)
    
    ggsurvplot(fit = km,
               data =data.rossi,
               conf.int = TRUE,
               title = "Curva de Supervivencia",
               xlab = "Semanas",
               ylab = "Probabilidad de no ser arrestado",
               legend.title = "Estimación",
               legend.labs = "Kaplan-Meier",
               cumcensor = T,
               ylim = c(0.65,1))
    
    ```
    


a. [10 puntos] Represente la función de riesgo acumulado no paramétrica de Nelson-Aalen.

    ```{r eval=T, include=T, echo=T, message=T}
    #Riesgo acumulado
    ggsurvplot(km,
               fun = "cumhaz",
               xlab = "Semanas",
               censor = T,
               ylab = "Riesgo Acumulado",
               title = "Riesgo Acumulado",
               legend.title = "Semanas sin ser arrestado")
    ```
    
a. [5 puntos] Represente nuevamente la función de sobrevivencia no paramétrica de Kaplan-Meier, pero ahora distinguiendo entre quienes recibieron y no recibieron ayuda del programa. ¿Qué observa?

    ```{r eval=T, include=T, echo=T, message=T}
    km.t <- survfit(Surv(week, arrest) ~ fin,
                     type = "kaplan-meier",
                     data=data.rossi)
    
    ggsurvplot(fit = km.t,
               data =data.rossi,
               conf.int = TRUE,
               title = "Curva de Supervivencia",
               xlab = "Semanas",
               ylab = "Probabilidad de no ser arrestado",
               legend.title = "Estimación",
               cumcensor = T,
               ylim = c(0.65,1))
    ```

a. [10 puntos] Estime ahora un modelo paramátrico de duración asumiendo una distribución Weibull. ¿Cómo cambia la tasa de riesgo cuando la edad se incrementa en un año? Recuerde la expresión para este cambio en el caso Weibull que vimos en la [clase 22](https://ecnii-2021.netlify.app/clases/clase_22.html#24).

    **Muy importante antes de que interprete los coeficientes**: note que en el paquete *survival* usa una parametrización de la distribución Weibull diferente a la que se presenta en Cameron y Trivedi (2005) y otros textos de economía. En particular, en clase vimos que con la parametrización Weibull, las funciones de riesgo y sobrevivencia son:

    $$\lambda(t)=\gamma \alpha t^{\alpha-1}$$

    $$S(t)=exp(-\Lambda(t))=exp(-\gamma t^{\alpha})$$


    A $\alpha$ se le conoce como parámetro de escala, mientras que $\gamma$ es la media. En cambio, en el paquete *survival* el parámetro de escala es $\sigma=\frac{1}{\alpha}$, y se hace $\mu/\sigma=-\ln \gamma$. Por tanto, 

    $$\lambda^*(t)=exp\left(-\frac{\mu}{\sigma}\right) \frac{1}{\sigma} t^{\frac{1}{\sigma}-1}$$
    $$S^*(t)=exp\left(-exp\left(-\frac{\mu}{\sigma}\right)t^{\frac{1}{\sigma}}\right)$$
    Esto implica que tenemos que *traducir* nuestros coeficientes estimados a la parametrización más natural, como en Cameron y Trivedi (2005), para poderlos interpretar. Basta entonces con calcular:

    $$\hat{\beta}_j=-\frac{\hat{\beta}_j^*}{\hat{\sigma}}$$
    donde $\hat{\beta}_j^*$ es el coeficiente estimado con *survival* y $\hat{\beta}_j$ es su contraparte asociada a la parametrización Weibull de Cameron y Trivedi (2005). Noten que $\hat{\sigma}$ es lo que la salida de la función *survreg* llama *Scale*. Podría hacer la transformación a mano, sin embargo, nos interesan también los errores estándar.

    Para hacer más práctica la transformación de los coeficientes Weibull use la función *ConvertWeibull* del paquete *SurvRegCensCov*. Esta función convierte los coeficientes a la parametrizaición Weibull más usada en economía, como la presentan Cameron y Trivedi (2005), y emplea el método delta para construir los errores estándar e intervalos de confianza. Ahora sí, puede interpretar los coeficientes de la regresión Weibull.

    *Estimamos la regresión Weibull y luego convertimos los coeficientes estimados a la parametrización vista en clase:*

    ```{r eval=T, include=T, echo=T, message=T}
    sreg <- survreg(Surv(week, arrest) ~ fin + age + race + wexp + mar + paro + prio,
                    data=data.rossi,
                    dist = "weibull")
    
    ConvertWeibull(sreg, conf.level = 0.95)
    ```
